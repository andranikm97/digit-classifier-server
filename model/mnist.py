# -*- coding: utf-8 -*-
"""MNIST NN (PyTorch).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SN5ij_rJSP2Zu8NBMtEGxDRbp1dKuztt
"""

import torch
from torch import nn, optim

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class NeuralNetwork(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2)
    self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)
    self.maxPool = nn.MaxPool2d(2)
    self.linear = nn.Linear(32 * 7 * 7, 10)
  
  def forward(self, x):
    h1 = self.maxPool(nn.functional.relu(self.conv1(x)))
    h2 = self.maxPool(nn.functional.relu(self.conv2(h1)))
    h3 = h2.view(h2.size(0), -1)
    out = self.linear(h3)
    return out

model = NeuralNetwork()
model.to(device)

params = model.parameters()
optimiser = optim.Adam(params, lr=0.01)

loss_fn = nn.CrossEntropyLoss()

def evaluate_model(model, data_loader):
  model.eval()
  
  losses = list()
  accuracies = list()

  for batch in data_loader:
    x, y = batch

    with torch.no_grad():
      logits = model(x.to(device))
    
      J = loss_fn(logits, y.to(device))

      losses.append(J.item())
      accuracies.append(y.eq(logits.detach().argmax(dim=1).cpu()).float().mean())

  loss = torch.tensor(losses).mean().float()
  accuracy = torch.tensor(accuracies).mean().float()
  model.train()
  return loss, accuracy


def train_model(model, X, Y):
  model.train()
  logits = model(X)
  J = loss_fn(logits, Y)
  model.zero_grad()
  J.backward()
  optimiser.step()
  return model
